{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROMPT\n",
    "\n",
    "Prompt:\n",
    "> I have a codebase that simulates graphs and estimates parameters using several modules. In this codebase, the following files provide the necessary functionality:\n",
    ">\n",
    "> - src/degrees_counts.py\n",
    "> Contains functions to compute the degree of a vertex and the sum of degrees for a given distance.\n",
    ">\n",
    "> - src/gic.py\n",
    "> Defines a GraphInformationCriterion class that computes the spectral density of graphs and calculates divergence measures (KL, L1, L2) between spectra.\n",
    ">\n",
    "> - src/graph.py\n",
    "> Implements a GraphModel class to generate graphs, add/remove edges using a logistic probability function, and methods to check convergence based on degree distribution, number of edges, and spectrum.\n",
    ">\n",
    "> - src/logit_estimator.py\n",
    "> Contains classes for parameter estimation using negative log-likelihood loss and logistic regression on graphs, along with functions for computing edge probabilities.\n",
    ">\n",
    "> - src/model_selection.py\n",
    "> Provides several model selection classes (e.g., RandomGraphModelSelector, ModelSelectorSpectrum, GraphModelSelection) which compare graph models such as ER, WS, BA, LG based on statistical properties (degree distribution, spectral density, etc.).\n",
    ">\n",
    "> - src/param_estimator.py\n",
    "> Implements GraphParameterEstimator which searches for optimal model parameters (using grid or ternary search) by minimizing the GIC.\n",
    ">\n",
    "> - src/utils.py\n",
    "> Contains helper functions for plotting graphs, spectra, degree distributions, and saving/loading graph artifacts.\n",
    ">\n",
    "> - notebooks/tests_dataset.py\n",
    "> Runs experiments with real connectome graphs from data files, applies the graph generation and edge-population methods, and plots the evolution of spectral distances and model selection results.\n",
    ">\n",
    "> Task to Implement:\n",
    ">\n",
    "> Using the above codebase as a foundation, please write code (or design a plan) that performs the following experimental study:\n",
    ">\n",
    "> 1. ANOVA Experiment on Graph Estimation:\n",
    "> - For each of the three different numbers of nodes: n = 100, 200, and 500, perform the following steps.\n",
    "> - For each n, simulate a graph using the available methods (for example, by using the populate_edges_spectrum method from GraphModel in src/graph.py).\n",
    "> - During each simulation, estimate the logistic regression parameter sigma (and compute its standard deviation and estimation values) using the methods provided in src/logit_estimator.py or related classes.\n",
    "> - Repeat this simulation 1000 times to collect a distribution of estimated sigma values for each graph size.\n",
    ">\n",
    "> 2. ANOVA Analysis:\n",
    "> - For the repeated simulations, perform an ANOVA analysis in a manner similar to how ANOVA is applied for logistic regression (i.e., comparing within-group and between-group variances).\n",
    "> - Consider two scenarios:\n",
    "> - Scenario A: The underlying model has two equal sigma parameters and a third different one (i.e., sigma1 = sigma2 ≠ sigma3).\n",
    "> - Scenario B: All three sigma values are equal (i.e., sigma1 = sigma2 = sigma3).\n",
    "> - Note that for Scenario B, you would expect the ANOVA to produce a diagonal ROC (i.e., no discrimination capability), while for Scenario A, the model should be able to detect significant differences.\n",
    ">\n",
    "> 3. ROC Curve Computation:\n",
    "> - Based on the results of the ANOVA tests (or p-values from the test), generate one or more ROC curves.\n",
    "> - The ROC curve should have:\n",
    "> - X-axis: Range of p-value thresholds.\n",
    "> - Y-axis: The number (or proportion) of rejections of the null hypothesis (i.e., cases where the test correctly identifies a significant difference).\n",
    "> - Comment on the expectation: with a high number of nodes, the ROC curve should show higher performance (better discrimination), whereas for the case with all sigmas equal, the ROC should be close to a diagonal line.\n",
    ">\n",
    "> Implementation Details and Hints:\n",
    ">\n",
    "> - You should re-use as much as possible from the provided modules (for example, use GraphModel for graph simulation, LogitRegEstimator or similar classes for parameter estimation, and the GIC methods to compare spectra).\n",
    "> - Use loops to vary n (nodes) and to perform 1000 simulation runs for each configuration.\n",
    "> - Collect the statistics (mean, standard deviation, and p-value from the ANOVA test) for each simulation set.\n",
    "> - After simulation, use an appropriate plotting library (for example, matplotlib) to generate the ROC curves, where each point corresponds to a p-value threshold and the corresponding rejection count.\n",
    "> - Include detailed logging or printouts of intermediate values (e.g., current sigma estimates, ANOVA statistics, etc.) for debugging and analysis.\n",
    "> - Optionally, integrate parts of the existing plotting functions from src/utils.py to visualize graphs and spectra if needed.\n",
    ">\n",
    "> Final Deliverable:\n",
    ">\n",
    "> Please provide a high-level design or pseudo-code (or a plan) outlining:\n",
    ">\n",
    "> - How the simulations will be structured,\n",
    "> - How the experiments for different n and sigma configurations will be organized,\n",
    "> - Which functions and classes from the provided codebase will be utilized/extended,\n",
    "> - How the ANOVA and ROC analysis will be executed,\n",
    "> - And any assumptions or modifications needed for coherence between the modules.\n",
    ">\n",
    "> End of Task.\n",
    ">\n",
    "> Feel free to ask clarifying questions if any part of the requirements is ambiguous.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Import from our codebase:\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphModel\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogit_estimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogitRegEstimator\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Simulation function for one run.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "High-Level Experiment: ANOVA on Graph Estimation\n",
    "\n",
    "For each n in [100, 200, 500], we simulate three sets of graphs (groups) where:\n",
    "    - In Scenario A: Two groups have sigma_true = 0.5 and one has sigma_true = 1.0.\n",
    "    - In Scenario B: All three groups have sigma_true = 0.5.\n",
    "    \n",
    "For each simulation replication:\n",
    "    1. For each group, generate a graph using GraphModel.populate_edges_spectrum.\n",
    "    2. Estimate sigma using LogitRegEstimator.\n",
    "    3. Collect estimated sigmas.\n",
    "    \n",
    "After many replications, perform ANOVA (using scipy.stats.f_oneway) on the three groups’ estimates,\n",
    "collect the p-value, and then compute an ROC-like curve by varying the p-value threshold.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Import from our codebase:\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.degrees_counts import compute_degree_distribution\n",
    "from src.graph import GraphModel\n",
    "from src.logit_estimator import LogitRegEstimator\n",
    "\n",
    "# -------------------------------\n",
    "# Simulation function for one run.\n",
    "# -------------------------------\n",
    "def run_simulation(n, sigma_true, d=1, warm_up=1000, max_iterations=5000, patience=10):\n",
    "    \"\"\"\n",
    "    Generate a graph with n nodes using the specified true sigma and estimate sigma.\n",
    "    - Uses GraphModel.populate_edges_spectrum (from src/graph.py) to simulate graph evolution.\n",
    "    - Uses LogitRegEstimator (from src/logit_estimator.py) to estimate sigma.\n",
    "    \n",
    "    Returns:\n",
    "        sigma_est: The estimated sigma value.\n",
    "    \"\"\"\n",
    "    # Initialize graph simulation with the true sigma value:\n",
    "    gm = GraphModel(n=n, d=d, sigma=sigma_true)\n",
    "    \n",
    "    # For simulation we use the current graph as the \"real_graph\" (this is an assumption \n",
    "    # for this experiment; the method may be adjusted if a different \"real\" graph should be used)\n",
    "    graphs, spec, spectrum_diffs, best_iteration = gm.populate_edges_spectrum(\n",
    "         warm_up=warm_up,\n",
    "         max_iterations=max_iterations,\n",
    "         patience=patience,\n",
    "         real_graph=gm.graph\n",
    "    )\n",
    "    simulated_graph = graphs[best_iteration]\n",
    "    \n",
    "    # Estimate sigma using logistic regression.\n",
    "    estimator_obj = LogitRegEstimator(simulated_graph, d=d)\n",
    "    features, labels = estimator_obj.get_features_labels()\n",
    "    # Here we use simple settings; parameters such as l1 weight and alpha can be tuned.\n",
    "    _, params, _ = estimator_obj.estimate_parameters(features, labels)# l1_wt=0, alpha=0)\n",
    "    sigma_est = params[0]  # As done in tests_dataset, sigma is taken as params[0]\n",
    "    return sigma_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Experiment function that collects sigma estimates for 3 groups.\n",
    "# -------------------------------\n",
    "def run_experiment(n, scenario, num_reps=1000, d=1, warm_up=1000, max_iterations=5000, patience=10):\n",
    "    \"\"\"\n",
    "    For a given graph size n and scenario, simulate replications to obtain sigma estimates.\n",
    "    \n",
    "    Parameters:\n",
    "      - n: number of nodes.\n",
    "      - scenario: \"A\" for sigma1=sigma2 != sigma3; \"B\" for sigma1 = sigma2 = sigma3.\n",
    "      - num_reps: Number of replications.\n",
    "    \n",
    "    Returns:\n",
    "      Three lists of estimated sigma (one per group).\n",
    "    \"\"\"\n",
    "    # Set true sigma values for the groups based on scenario:\n",
    "    if scenario == \"A\":\n",
    "        sigma_group1 = 0.5\n",
    "        sigma_group2 = 0.5\n",
    "        sigma_group3 = 1.0  # different value\n",
    "    elif scenario == \"B\":\n",
    "        sigma_group1 = sigma_group2 = sigma_group3 = 0.5\n",
    "    else:\n",
    "        raise ValueError(\"Scenario must be either 'A' or 'B'.\")\n",
    "    \n",
    "    estimates_g1, estimates_g2, estimates_g3 = [], [], []\n",
    "    \n",
    "    for rep in range(num_reps):\n",
    "        sigma_est1 = run_simulation(n, sigma_group1, d, warm_up, max_iterations, patience)\n",
    "        sigma_est2 = run_simulation(n, sigma_group2, d, warm_up, max_iterations, patience)\n",
    "        sigma_est3 = run_simulation(n, sigma_group3, d, warm_up, max_iterations, patience)\n",
    "        estimates_g1.append(sigma_est1)\n",
    "        estimates_g2.append(sigma_est2)\n",
    "        estimates_g3.append(sigma_est3)\n",
    "\n",
    "        if rep % 50 == 0:\n",
    "            print(f\"n={n}, scenario={scenario}: Completed {rep} replications\")\n",
    "    \n",
    "    # You can print summary statistics if needed:\n",
    "    print(f\"Group means for n={n}, scenario={scenario}:\",\n",
    "          np.mean(estimates_g1), np.mean(estimates_g2), np.mean(estimates_g3))\n",
    "    \n",
    "    # Return the three groups of sigma estimates.\n",
    "    return estimates_g1, estimates_g2, estimates_g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Function to compute ROC curve (rejection rate vs. p-value threshold)\n",
    "# -------------------------------\n",
    "def compute_roc_curve(p_values, thresholds=np.linspace(0, 1, 101)):\n",
    "    \"\"\"\n",
    "    Given a collection of ANOVA p-values across experiments, compute for each p-value threshold\n",
    "    the proportion of experiments where the null hypothesis would be rejected.\n",
    "    \n",
    "    Returns:\n",
    "       thresholds: array of p-value thresholds.\n",
    "       rejection_rates: corresponding rejection rates (percent of p-values below each threshold).\n",
    "    \"\"\"\n",
    "    rejection_rates = []\n",
    "    for thresh in thresholds:\n",
    "        rejections = np.mean(np.array(p_values) < thresh)\n",
    "        rejection_rates.append(rejections)\n",
    "    return thresholds, rejection_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiments for n=100 in Scenario A...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning experiments for n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in Scenario \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscenario\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_experiments):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Run a smaller sub-experiment to generate three groups of sigma estimates:\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     g1, g2, g3 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m(n, scenario, num_reps\u001b[38;5;241m=\u001b[39mreps_per_experiment, d\u001b[38;5;241m=\u001b[39md, warm_up\u001b[38;5;241m=\u001b[39mwarm_up, max_iterations\u001b[38;5;241m=\u001b[39mmax_iterations, patience\u001b[38;5;241m=\u001b[39mpatience)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Perform one-way ANOVA on the three groups.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     f_val, p_val \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mf_oneway(g1, g2, g3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main routine: Loop over n sizes and both scenarios,\n",
    "# collecting p-values from repeated ANOVA tests.\n",
    "# -------------------------------\n",
    "#ns = [100, 200, 500]\n",
    "ns = [100]\n",
    "#scenarios = [\"A\", \"B\"] # A is the scenario where the sigma values are different B is the scenario where the sigma values are the same\n",
    "scenarios = [\"A\"]\n",
    "\n",
    "# Set number of experiments – each experiment groups several simulation replications.\n",
    "num_experiments =10 \n",
    "reps_per_experiment = 5 \n",
    "\n",
    "# LG params\n",
    "d = 0\n",
    "warm_up = 1000\n",
    "max_iterations = 5000\n",
    "patience = 10\n",
    "\n",
    "# Dictionary to store ANOVA p-values for each combination:\n",
    "results = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    results[scenario] = {}\n",
    "    for n in ns:\n",
    "        exp_p_values = []\n",
    "        print(f\"\\nRunning experiments for n={n} in Scenario {scenario}...\")\n",
    "        for exp in range(num_experiments):\n",
    "            # Run a smaller sub-experiment to generate three groups of sigma estimates:\n",
    "            g1, g2, g3 = run_experiment(n, scenario, num_reps=reps_per_experiment, d=d, warm_up=warm_up, max_iterations=max_iterations, patience=patience)\n",
    "            # Perform one-way ANOVA on the three groups.\n",
    "            f_val, p_val = stats.f_oneway(g1, g2, g3)\n",
    "            exp_p_values.append(p_val)\n",
    "            print(f\"  Experiment {exp+1}/{num_experiments}: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "        results[scenario][n] = exp_p_values\n",
    "\n",
    "        # Compute and plot the ROC curve (rejection rate vs. threshold) for this configuration.\n",
    "        thresholds, rejection_rates = compute_roc_curve(exp_p_values)\n",
    "        plt.plot(thresholds, rejection_rates, label=f\"n={n}, scenario={scenario}\")\n",
    "\n",
    "# Plot the ROC curves for all configurations.\n",
    "plt.xlabel(\"P-value threshold\")\n",
    "plt.ylabel(\"Rejection Rate\")\n",
    "plt.title(\"ROC Curve: ANOVA Rejection Rate vs. P-value Threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
